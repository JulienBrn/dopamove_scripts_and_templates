{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, xarray as xr\n",
    "from pathlib import Path\n",
    "import datetime, networkx as nx, yaml, warnings, sys, logging\n",
    "from disjoint_set import DisjointSet\n",
    "from helper import singleglob, nxrender, Step, dict_merge, PdfWriter, DictWriter, TableWriter, TextWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl, seaborn as sns\n",
    "mpl.rcParams['figure.figsize'] = [15, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_name = Path(\"events.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = Path(\".\").resolve().parent\n",
    "result_folder = Path(\".\")/notebook_name.stem\n",
    "result_folder.mkdir(exist_ok=True, parents=True)\n",
    "node_metadata_step = Step(Path(\".\").resolve() / \"task_graph.ipynb\")\n",
    "metadata_path = singleglob(base_folder, \"metadata.yaml\", \"metadata --*.yaml\", \"metadata--*.yaml\")\n",
    "poly_dat_path = singleglob(base_folder, \"poly/events*.dat\")\n",
    "fiber_events_path = singleglob(base_folder, \"**/Events.csv\", \"**/Events --*.csv\", \"**/Events--*.csv\")\n",
    "trial_dataset_path =  result_folder / \"trial_events.nc\"\n",
    "base_folder, node_metadata_step, metadata_path, poly_dat_path, fiber_events_path, trial_dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = TableWriter(result_folder/\"tables.xlsx\")\n",
    "figures = PdfWriter(result_folder/\"figures.pdf\")\n",
    "dicts = DictWriter(result_folder/\"dicts.yaml\")\n",
    "notebook_save_path = result_folder/\"notebook.html\"\n",
    "warn =  TextWriter(result_folder/\"warnings.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metadata = yaml.safe_load(metadata_path.open(\"r\"))\n",
    "event_metadata = all_metadata[\"task\"][\"events\"]\n",
    "dicts.write(event_metadata = event_metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Poly events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_metadata = yaml.safe_load((node_metadata_step.exec_if_necessary()/\"node_metadata.yaml\").open(\"r\"))\n",
    "dicts.write(node_metadata = node_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_event_df = pd.read_csv(poly_dat_path, sep=\"\\t\", names=['time (ms)', 'family', 'nbre', '_P', '_V', '_L', '_R', '_T', '_W', '_X', '_Y', '_Z'], skiprows=13)\n",
    "poly_event_df.insert(0, \"t\", poly_event_df[\"time (ms)\"]/1000)\n",
    "if \"valid_recording_intervals\" in all_metadata[\"recordings\"][\"poly\"]:\n",
    "    l = all_metadata[\"recordings\"][\"poly\"][\"valid_recording_intervals\"]\n",
    "    if len(l) > 1:\n",
    "        raise Exception(f\"For now, only one valid recording interval allowed\")\n",
    "    start = l[0][\"start\"]\n",
    "    end = l[0][\"end\"]\n",
    "    if not start:\n",
    "        start = -np.inf\n",
    "    if not end:\n",
    "        end=np.inf\n",
    "    poly_event_df = poly_event_df.loc[(poly_event_df[\"t\"] >= start)&(poly_event_df[\"t\"] <= end)]\n",
    "        \n",
    "poly_event_df = poly_event_df.sort_values(\"t\")\n",
    "poly_event_df[\"state_count\"] = (poly_event_df[\"family\"]==10).cumsum()\n",
    "poly_event_df[\"curr_node\"] = poly_event_df[\"_T\"].where(poly_event_df[\"family\"]==10).ffill()\n",
    "print(tables.write(poly_event_df = poly_event_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_grp(grp: pd.DataFrame):\n",
    "    s, n = grp.name\n",
    "    metadata = node_metadata[n] if n in node_metadata else {}\n",
    "    t_start = grp[\"t\"].min()\n",
    "    has_pause = len(grp.loc[grp[\"family\"]==11].index) > 0\n",
    "    event = metadata[\"event\"] if \"event\" in metadata else None\n",
    "    return pd.Series(dict(t_start=t_start, event_name=event, metadata=metadata, has_pause=has_pause))\n",
    "\n",
    "poly_processed_events_df= poly_event_df.groupby([\"state_count\", \"curr_node\"]).apply(reduce_grp, include_groups=False).sort_values(\"t_start\").reset_index()\n",
    "poly_processed_events_df.insert(3, \"t_end\", poly_processed_events_df[\"t_start\"].shift(-1))\n",
    "print(tables.write(poly_processed_events_df = poly_processed_events_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_poly_events = poly_processed_events_df.copy().dropna(subset=\"event_name\")\n",
    "final_poly_events[\"metadata\"] = final_poly_events.apply(lambda row: dict_merge(row[\"metadata\"], dict(poly=dict(pause=row[\"has_pause\"], curr_node=row[\"curr_node\"]))), axis=1)\n",
    "final_poly_events = final_poly_events.drop(columns=[\"state_count\", \"curr_node\", \"t_end\", \"has_pause\"])\n",
    "final_poly_events[\"event_id\"] = np.arange(len(final_poly_events.index))\n",
    "print(tables.write(final_poly_events = final_poly_events))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling of Fiber Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_fiber_events = pd.read_csv(fiber_events_path).sort_values(\"TimeStamp\")\n",
    "raw_fiber_events.insert(0, \"t\", raw_fiber_events[\"TimeStamp\"]/1000)\n",
    "if \"valid_recording_intervals\" in all_metadata[\"recordings\"][\"fiber\"]:\n",
    "    l = all_metadata[\"recordings\"][\"fiber\"][\"valid_recording_intervals\"]\n",
    "    if len(l) > 1:\n",
    "        raise Exception(f\"For now, only one valid recording interval allowed\")\n",
    "    start = l[0][\"start\"]\n",
    "    end = l[0][\"end\"]\n",
    "    if not start:\n",
    "        start = -np.inf\n",
    "    if not end:\n",
    "        end=np.inf\n",
    "    raw_fiber_events = raw_fiber_events.loc[(raw_fiber_events[\"t\"] >= start)&(raw_fiber_events[\"t\"] <= end)]\n",
    "tables.write(raw_fiber_events=raw_fiber_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_fiber_events[\"ev_num\"] = raw_fiber_events.groupby([\"Name\", \"State\"]).cumcount()\n",
    "fiber_events=raw_fiber_events.set_index([\"ev_num\", \"Name\", \"State\"])[\"t\"].unstack(\"State\").reset_index().rename(columns={0:\"t_start\", 1:\"t_end\"}).drop(columns=\"ev_num\")\n",
    "fiber_events.columns.name=None\n",
    "tables.write(fiber_events=fiber_events.sort_values(\"t_start\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fiber_event(name, start, end):\n",
    "    event = None\n",
    "    metadata = None\n",
    "    duration = end-start\n",
    "    for d in event_metadata[\"fiber\"]:\n",
    "        detect = d[\"detection\"]\n",
    "        desc = d[\"description\"]\n",
    "        selected=True\n",
    "        if \"name\" in detect:\n",
    "            if name != detect[\"name\"]:\n",
    "                selected=False\n",
    "        if \"duration_min\" in detect:\n",
    "            if detect[\"duration_min\"] > duration:\n",
    "                selected=False\n",
    "        if \"duration_max\" in detect:\n",
    "            if detect[\"duration_max\"] < duration:\n",
    "                selected=False\n",
    "        # print(detect, name, duration)\n",
    "        if selected:\n",
    "            if event is not None or metadata is not None:\n",
    "                raise Exception(f\"conflict {event} {metadata}, {detect}, {duration}\")\n",
    "            event = desc[\"event\"]\n",
    "            metadata = desc\n",
    "    return (event, metadata)\n",
    "processed_fiber_events = fiber_events.copy()\n",
    "processed_fiber_events[[\"event_name\", \"metadata\"]] = processed_fiber_events.apply(lambda row: process_fiber_event(row[\"Name\"], row[\"t_start\"], row[\"t_end\"]), axis=1, result_type=\"expand\")\n",
    "tables.write(processed_fiber_events=processed_fiber_events.sort_values(\"t_start\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fiber_events = processed_fiber_events.copy().dropna(subset=\"event_name\").sort_values(\"t_start\")\n",
    "final_fiber_events[\"metadata\"] = final_fiber_events.apply(lambda row: dict_merge(row[\"metadata\"], dict(fiber=dict(FiberInputNum=row[\"Name\"]))), axis=1)\n",
    "final_fiber_events = final_fiber_events.drop(columns=[\"t_end\", \"Name\"])\n",
    "final_fiber_events[\"event_id\"] = np.arange(len(final_fiber_events.index))\n",
    "tables.write(final_fiber_events=final_fiber_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronizing and merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\"fiber\": final_fiber_events, \"poly\": final_poly_events}\n",
    "n_evs = 0\n",
    "for k, v in df_dict.items():\n",
    "    df_dict[k] = v.assign(event_id=np.arange(n_evs, n_evs+len(v.index)), source=k)\n",
    "    n_evs+=len(v.index)\n",
    "dfs = pd.Series(df_dict, name=\"df\").to_frame()\n",
    "dfs.index.name=\"source\"\n",
    "merge_data = dfs.to_xarray()\n",
    "event_name_list = xr.apply_ufunc(lambda df: set(df[\"event_name\"].dropna().to_list()), merge_data[\"df\"], vectorize=True)\n",
    "merge_data[\"event\"] = xr.DataArray(list(set.union(*list(event_name_list.to_numpy()))), dims=\"event\")\n",
    "merge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data[\"event_df\"] = xr.apply_ufunc(lambda df, event: {\"df\": df.loc[df[\"event_name\"] == event]}, merge_data[\"df\"], merge_data[\"event\"], vectorize=True)\n",
    "merge_data[\"num_events\"] = xr.apply_ufunc(lambda df: len(df[\"df\"].index), merge_data[\"event_df\"], vectorize=True)\n",
    "merge_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing t_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_time_source = \"fiber\"\n",
    "shift_using = \"cue\"\n",
    "\n",
    "if not (merge_data[\"num_events\"].sel(event=shift_using) == merge_data[\"num_events\"].sel(source=base_time_source, event=shift_using)).all():\n",
    "    raise Exception(f\"Problem not same number of {shift_using} event\")\n",
    "\n",
    "def get_shift(df, target_df):\n",
    "    df = df[\"df\"]\n",
    "    target_df = target_df[\"df\"]\n",
    "    res_df = pd.DataFrame()\n",
    "    res_df[\"t\"] = df[\"t_start\"].to_numpy()\n",
    "    res_df[\"shift\"] = df[\"t_start\"].to_numpy() - target_df[\"t_start\"].to_numpy()\n",
    "    res_df = pd.concat([pd.DataFrame([dict(t=-np.inf, shift=res_df[\"shift\"].iat[0])]), res_df])\n",
    "    return {\"df\":res_df}\n",
    "\n",
    "tmp =merge_data[\"event_df\"].sel(source=base_time_source, event=shift_using, drop=True)\n",
    "tmp2 =  merge_data[\"event_df\"].sel(event=shift_using, drop=True)\n",
    "merge_data[\"shift_data\"] = xr.apply_ufunc(get_shift, tmp2, tmp, vectorize=True)\n",
    "\n",
    "shift_data_dfs = {merge_data[\"source\"].isel(source=i).item(): merge_data[\"shift_data\"].isel(source=i).item()[\"df\"] for i in range(merge_data.sizes[\"source\"])}\n",
    "tables.write(**{f\"shift_data_{k}\":v for k,v in shift_data_dfs.items()})\n",
    "\n",
    "def shift_data(df, shift_data):\n",
    "    shift_data = shift_data[\"df\"]\n",
    "    res = df.copy()\n",
    "    res.insert(1, \"t_shifted\", res[\"t_start\"].to_numpy() - shift_data[\"shift\"].iloc[np.searchsorted(shift_data[\"t\"], res[\"t_start\"], side=\"right\")-1].to_numpy())\n",
    "    return dict(df=res)\n",
    "\n",
    "merge_data[\"shifted_df\"] = xr.apply_ufunc(shift_data, merge_data[\"df\"], merge_data[\"shift_data\"], vectorize=True)\n",
    "display(merge_data)\n",
    "all_events_df = pd.concat([v[\"df\"] for v in merge_data[\"shifted_df\"].to_numpy()]).sort_values(\"t_shifted\")\n",
    "all_events_df[\"metadata\"] = all_events_df.pop(\"metadata\")\n",
    "tables.write(all_events_df=all_events_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_sets = DisjointSet.from_iterable(all_events_df[\"event_id\"].to_numpy())\n",
    "for ev, g in all_events_df.groupby(\"event_name\"):\n",
    "    sources = g.groupby(\"source\")\n",
    "    for s1, g1 in sources:\n",
    "        for s2, g2 in sources:\n",
    "            if len(g1.index) == 0 or len(g2.index) == 0:\n",
    "                continue\n",
    "            elif len(g1.index) == len(g2.index):\n",
    "                for id1, id2 in zip(g1[\"event_id\"], g2[\"event_id\"]):\n",
    "                    ev_sets.union(id1, id2)\n",
    "            else:\n",
    "                if len(g1.index)  < len(g2.index):\n",
    "                    tmp = g1\n",
    "                    g1 = g2\n",
    "                    g2=g1\n",
    "                g1_ar = xr.DataArray(g1[\"event_id\"].to_numpy(), dims=\"t\", coords=dict(t=g1[\"t_shifted\"].to_numpy()))\n",
    "                match = g1_ar.sel(t=g2[\"t_shifted\"].to_numpy(), method=\"nearest\").to_numpy()\n",
    "                for id1, id2 in zip(match, g2[\"event_id\"]):\n",
    "                    ev_sets.union(id1, id2)\n",
    "ev_sets = list(ev_sets.itersets())\n",
    "print(\" ; \".join([(\"\\n\" if i % 12==0 and i> 0 else \"\") + str(s).replace(\" \", \"\") for i, s in enumerate(ev_sets)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_priority = [\"fiber\", \"poly\"]\n",
    "merged_ev_dict = []\n",
    "dfs = []\n",
    "for i,s in enumerate(ev_sets):\n",
    "    df = all_events_df.loc[all_events_df[\"event_id\"].isin(s)]\n",
    "    metadata = dict_merge(*df[\"metadata\"].to_list())\n",
    "    ev_name = df[\"event_name\"].iat[0]\n",
    "    t = df.sort_values(\"source\", key=lambda src: src.map({k:i for i,k in enumerate(t_priority)}))[\"t_shifted\"].iat[0]\n",
    "    sources = \"+\".join(df[\"source\"].to_list())\n",
    "    merged_ev_dict.append(dict(merged_id=i, event_ids=s, event_name=ev_name, t=t, sources=sources, metadata=metadata))\n",
    "    dfs.append(df.assign(merged_id=i, event_name=ev_name))\n",
    "merge_events_df = pd.DataFrame(merged_ev_dict)\n",
    "all_merged_dataset = pd.concat(dfs).set_index([\"source\", \"merged_id\"]).to_xarray()\n",
    "all_merged_dataset[\"event_name\"] = all_merged_dataset[\"event_name\"].isel(source=0)\n",
    "all_merged_dataset = all_merged_dataset.set_coords(\"event_name\")\n",
    "display(all_merged_dataset)\n",
    "tables.write(merge_events_df=merge_events_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alterated = []\n",
    "for i in range(all_merged_dataset.sizes[\"source\"]):\n",
    "    s = all_merged_dataset[\"source\"].isel(source=i).item()\n",
    "    d = all_merged_dataset.isel(source=i)\n",
    "    d = d.where(d[\"event_id\"].notnull(), drop=True)\n",
    "    new_ev_order = d.sortby(\"t_shifted\")[\"event_id\"].to_numpy() \n",
    "    initial_ev_order= d.sortby(\"t_start\")[\"event_id\"].to_numpy()\n",
    "    if not (new_ev_order == initial_ev_order).all():\n",
    "        alterated.append(s)\n",
    "if len(alterated) > 0:\n",
    "    warnings.warn(warn.write(f'Merge warning: Shifting alters order for sources {alterated}'))\n",
    "else:\n",
    "    print(\"All ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(all_merged_dataset.sizes[\"source\"]):\n",
    "    for j in range(i+1, all_merged_dataset.sizes[\"source\"]):\n",
    "        s1 = all_merged_dataset[\"source\"].isel(source=i).item()\n",
    "        s2 = all_merged_dataset[\"source\"].isel(source=j).item()\n",
    "        d = (all_merged_dataset[\"t_shifted\"].isel(source=i) - all_merged_dataset[\"t_shifted\"].isel(source=j)).rename(\"delta_t\").to_dataframe()\n",
    "        min = d[\"delta_t\"].dropna().min()\n",
    "        max = d[\"delta_t\"].dropna().max()\n",
    "        if min < -0.020 or max > 0.020:\n",
    "            warnings.warn(warn.write(f\"Merge interval is between source {s1} ans {s2} is [{min}, {max}]\"))\n",
    "        bins = [-0.020, -0.010, -0.005, -0.002, 0.002, 0.005, 0.010, 0.020]\n",
    "        bins = [b for b in bins if b>min and b<max]\n",
    "        bins = [min] + bins + [max]\n",
    "        sns.displot(d, x=\"delta_t\", hue=\"event_name\", bins=bins, multiple=\"dodge\", stat=\"probability\", common_norm=False, shrink=.8)\n",
    "        \n",
    "        plt.xticks(bins, labels = [f'{b:.2g}' for b in bins], rotation=25)\n",
    "        plt.suptitle(f't_shifted variation between {s1} and {s2}')\n",
    "        figures.write()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min=np.inf\n",
    "max = -np.inf\n",
    "for i in range(merge_data.sizes[\"source\"]):\n",
    "    s = merge_data[\"source\"].isel(source=i).item()\n",
    "    shift_df = merge_data[\"shift_data\"].isel(source=i).item()[\"df\"]\n",
    "    c_min = shift_df[\"shift\"].min()\n",
    "    c_max = shift_df[\"shift\"].max()\n",
    "    if c_min < min:\n",
    "        min = c_min\n",
    "    if c_max > max:\n",
    "        max = c_max\n",
    "    plt.stairs(shift_df[\"shift\"].to_numpy()[:-1],shift_df[\"t\"].to_numpy(), color=f\"C{i}\", baseline=None, label = s)\n",
    "plt.suptitle(f'Shift amount over time\\nAlignment is done on {shift_using} events. Common time is that of {base_time_source}')\n",
    "plt.xlabel(\"t\\nin time of source\")\n",
    "plt.ylabel(f\"shift amount\\nt_common = t_source - shift\")\n",
    "plt.legend(title=\"source\")\n",
    "plt.ylim([min - (max-min)*0.05, max + (max-min)*0.05])\n",
    "figures.write()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing to trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_evs = set(merge_events_df[\"event_name\"].dropna().to_list())\n",
    "trial_type_cols = set.union(*[set(d[\"trial_type\"].keys()) for d in merge_events_df[\"metadata\"].values if \"trial_type\" in d])\n",
    "def reduce_trial(grp: pd.DataFrame): \n",
    "    trial = grp.name\n",
    "    prev_trial_end = grp[\"t\"].min()\n",
    "    real_start = grp.loc[grp[\"event_name\"]==\"trial_start\", \"t\"].max()\n",
    "    first_start = grp.loc[grp[\"event_name\"]==\"trial_start\", \"t\"].min()\n",
    "    if pd.isna(real_start):\n",
    "        real_grp = grp\n",
    "    else:\n",
    "        real_grp = grp.loc[grp[\"t\"] >=real_start]\n",
    "    events = real_grp[[\"event_name\", \"t\"]].dropna(subset=\"event_name\").set_index(\"event_name\")[\"t\"]\n",
    "    event_dict = {k:events[k] if k in events else pd.NA for k in all_evs}\n",
    "    if events.index.duplicated().any():\n",
    "        raise Exception(\"Duplicated events\")\n",
    "    \n",
    "    metadata = dict_merge(*real_grp[\"metadata\"].to_list(), incompatible=\"remove\")\n",
    "    trial_type = {k:metadata[\"trial_type\"][k] if \"trial_type\"  in metadata and k in metadata[\"trial_type\"] else pd.NA for k in trial_type_cols }\n",
    "    res = {\n",
    "           \"events\": pd.DataFrame([event_dict | dict(first_start=first_start, prev_trial_end = prev_trial_end)]),\n",
    "           \"trial_type\": pd.DataFrame([trial_type]),\n",
    "    }\n",
    "    r = pd.concat(res, axis=1)\n",
    "    r[\"trial\"] = trial\n",
    "    return r\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning) # Seems to be a pandas bug, see https://github.com/pandas-dev/pandas/issues/55928\n",
    "    trial_event_df = merge_events_df.groupby((merge_events_df[\"event_name\"]==\"trial_end\").cumsum()-1).apply(reduce_trial).reset_index(drop=True)\n",
    "\n",
    "trial_event_df[(\"events\", \"trial_end\")] = trial_event_df.pop((\"events\", \"prev_trial_end\")).shift(-1)\n",
    "trial_event_df=trial_event_df.set_index(\"trial\")\n",
    "\n",
    "tables.write(trial_event_df=trial_event_df)\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_dataset = xr.merge([trial_event_df[\"events\"].rename_axis('event_name', axis=1).stack().rename(\"event_t\").to_xarray(), trial_event_df[\"trial_type\"].to_xarray()])\n",
    "trial_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_dataset.fillna(np.nan).to_netcdf(trial_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.load_dataset(trial_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(f'jupyter nbconvert --to html {notebook_name} --output {notebook_save_path} --no-prompt --Application.log_level=40')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tables\n",
    "del dicts\n",
    "del figures\n",
    "del warn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
