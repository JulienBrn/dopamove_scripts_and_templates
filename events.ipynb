{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, xarray as xr\n",
    "from pathlib import Path\n",
    "import datetime, networkx as nx, yaml\n",
    "from helper import singleglob, nxrender, Step, dict_merge\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl, seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = Path(\".\").resolve().parent.parent\n",
    "node_metadata_step = Step(Path(\".\").resolve().parent / \"task_graph\" / \"task_graph.ipynb\",  \"node_metadata.yaml\")\n",
    "metadata_path = singleglob(base_folder, \"metadata.yaml\", \"metadata --*.yaml\", \"metadata--*.yaml\")\n",
    "poly_dat_path = singleglob(base_folder, \"*.dat\")\n",
    "fiber_events_path = singleglob(base_folder, \"**/Events.csv\", \"**/Events --*.csv\", \"**/Events--*.csv\")\n",
    "base_folder, node_metadata_step, metadata_path, poly_dat_path, fiber_events_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_metadata = yaml.safe_load(metadata_path.open(\"r\"))[\"task\"][\"events\"]\n",
    "event_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Poly events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_metadata = yaml.safe_load(node_metadata_step.exec_if_necessary().open(\"r\"))\n",
    "node_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_event_df = pd.read_csv(poly_dat_path, sep=\"\\t\", names=['time (ms)', 'family', 'nbre', '_P', '_V', '_L', '_R', '_T', '_W', '_X', '_Y', '_Z'], skiprows=13)\n",
    "poly_event_df.insert(0, \"t\", poly_event_df[\"time (ms)\"]/1000)\n",
    "poly_event_df = poly_event_df.sort_values(\"t\")\n",
    "print(poly_event_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_event_df[\"state_count\"] = (poly_event_df[\"family\"]==10).cumsum()\n",
    "poly_event_df[\"curr_node\"] = poly_event_df[\"_T\"].where(poly_event_df[\"family\"]==10).ffill()\n",
    "print(poly_event_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_grp(grp: pd.DataFrame):\n",
    "    s, n = grp.name\n",
    "    metadata = node_metadata[n] if n in node_metadata else {}\n",
    "    t_start = grp[\"t\"].min()\n",
    "    has_pause = len(grp.loc[grp[\"family\"]==11].index) > 0\n",
    "    event = metadata[\"event\"] if \"event\" in metadata else None\n",
    "    return pd.Series(dict(t_start=t_start, event_name=event, metadata=metadata, has_pause=has_pause))\n",
    "\n",
    "poly_processed_events_df= poly_event_df.groupby([\"state_count\", \"curr_node\"]).apply(reduce_grp).sort_values(\"t_start\").reset_index()\n",
    "poly_processed_events_df.insert(3, \"t_end\", poly_processed_events_df[\"t_start\"].shift(-1))\n",
    "poly_processed_events_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_poly_events = poly_processed_events_df.copy().dropna(subset=\"event_name\")\n",
    "final_poly_events[\"metadata\"] = final_poly_events.apply(lambda row: dict_merge(row[\"metadata\"], dict(poly=dict(pause=row[\"has_pause\"], curr_node=row[\"curr_node\"]))), axis=1)\n",
    "final_poly_events = final_poly_events.drop(columns=[\"state_count\", \"curr_node\", \"t_end\", \"has_pause\"])\n",
    "final_poly_events[\"event_id\"] = np.arange(len(final_poly_events.index))\n",
    "print(final_poly_events.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling of Fiber Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_fiber_events = pd.read_csv(fiber_events_path).sort_values(\"TimeStamp\")\n",
    "raw_fiber_events.insert(0, \"t\", raw_fiber_events[\"TimeStamp\"]/1000)\n",
    "print(raw_fiber_events.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_fiber_events[\"ev_num\"] = raw_fiber_events.groupby([\"Name\", \"State\"]).cumcount()\n",
    "fiber_events=raw_fiber_events.set_index([\"ev_num\", \"Name\", \"State\"])[\"t\"].unstack(\"State\").reset_index().rename(columns={0:\"t_start\", 1:\"t_end\"}).drop(columns=\"ev_num\")\n",
    "fiber_events.columns.name=None\n",
    "print(fiber_events.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fiber_event(name, start, end):\n",
    "    event = None\n",
    "    metadata = None\n",
    "    duration = end-start\n",
    "    for d in event_metadata[\"fiber\"]:\n",
    "        detect = d[\"detection\"]\n",
    "        desc = d[\"description\"]\n",
    "        selected=True\n",
    "        if \"name\" in detect:\n",
    "            if name != detect[\"name\"]:\n",
    "                selected=False\n",
    "        if \"duration_min\" in detect:\n",
    "            if detect[\"duration_min\"] > duration:\n",
    "                selected=False\n",
    "        if \"duration_max\" in detect:\n",
    "            if detect[\"duration_max\"] < duration:\n",
    "                selected=False\n",
    "        # print(detect, name, duration)\n",
    "        if selected:\n",
    "            if event is not None or metadata is not None:\n",
    "                raise Exception(f\"conflict {event} {metadata}, {detect}, {duration}\")\n",
    "            event = desc[\"event\"]\n",
    "            metadata = desc\n",
    "    return (event, metadata)\n",
    "processed_fiber_events = fiber_events.copy()\n",
    "processed_fiber_events[[\"event_name\", \"metadata\"]] = processed_fiber_events.apply(lambda row: process_fiber_event(row[\"Name\"], row[\"t_start\"], row[\"t_end\"]), axis=1, result_type=\"expand\")\n",
    "processed_fiber_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fiber_events = processed_fiber_events.copy().dropna(subset=\"event_name\").sort_values(\"t_start\")\n",
    "final_fiber_events[\"metadata\"] = final_fiber_events.apply(lambda row: dict_merge(row[\"metadata\"], dict(fiber=dict(FiberInputNum=row[\"Name\"]))), axis=1)\n",
    "final_fiber_events = final_fiber_events.drop(columns=[\"t_end\", \"Name\"])\n",
    "final_fiber_events[\"event_id\"] = np.arange(len(final_fiber_events.index))\n",
    "final_fiber_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronizing and merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_shift=False\n",
    "dfs = pd.Series({\"fiber\": final_fiber_events, \"poly\": final_poly_events}, name=\"df\").to_frame()\n",
    "dfs.index.name=\"source\"\n",
    "merge_data = dfs.to_xarray()\n",
    "merge_data[\"event_list\"] = xr.apply_ufunc(lambda df: set(df[\"event_name\"].dropna().to_list()), merge_data[\"df\"], vectorize=True)\n",
    "merge_data[\"event\"] = xr.DataArray(list(set.union(*list(merge_data[\"event_list\"].to_numpy()))), dims=\"event\")\n",
    "merge_data[\"event_df\"] = xr.apply_ufunc(lambda df, ev: df.loc[df[\"event_name\"] == ev], merge_data[\"df\"], merge_data[\"event\"], vectorize=True)\n",
    "merge_data[\"num_values\"] = xr.apply_ufunc(lambda df: len(df.index), merge_data[\"event_df\"], vectorize=True)\n",
    "merge_ok = ((merge_data[\"num_values\"] == merge_data[\"num_values\"].isel(source=0)) | (merge_data[\"num_values\"]==0))\n",
    "if not merge_ok.all():\n",
    "    print(f'Possible problem for merge, problem matching number of events...')\n",
    "    display(merge_ok)\n",
    "merge_data[\"avg_value\"] = xr.apply_ufunc(lambda df: df[\"t_start\"].mean(), merge_data[\"event_df\"], vectorize=True)\n",
    "merge_data[\"var_value\"] = xr.apply_ufunc(lambda df: df[\"t_start\"].var(), merge_data[\"event_df\"], vectorize=True)\n",
    "merge_data[\"shift_value\"] = (merge_data[\"avg_value\"] - merge_data[\"avg_value\"].isel(source=0)).where(merge_ok)\n",
    "if single_shift:\n",
    "    merge_data[\"applied_shift_value\"] = merge_data[\"shift_value\"].median(\"event\")\n",
    "else:\n",
    "    merge_data[\"applied_shift_value\"] = xr.where(merge_data[\"shift_value\"].notnull(), merge_data[\"shift_value\"], merge_data[\"shift_value\"].median(\"event\"))\n",
    "merge_data[\"shifted_event_df\"] = xr.apply_ufunc(lambda df, shift: df.copy().assign(t_shifted=df[\"t_start\"]-shift).sort_values(\"t_shifted\"), merge_data[\"event_df\"], merge_data[\"applied_shift_value\"], vectorize=True)\n",
    "merge_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def match_event_df(df: pd.DataFrame, target_df: pd.DataFrame):\n",
    "    res = df.copy()\n",
    "    if len(target_df.index) == 0: \n",
    "        res[\"match\"] = pd.NA\n",
    "    elif len(target_df.index) == len(df.index):\n",
    "        res[\"match\"] = target_df[\"event_id\"].to_numpy()\n",
    "    elif len(target_df.index) > len(df.index):\n",
    "        target_ar = xr.DataArray(target_df[\"event_id\"].to_numpy(), dims=\"t\", coords=dict(t=target_df[\"t_shifted\"].to_numpy()))\n",
    "        res[\"match\"] = target_ar.sel(t=res[\"t_shifted\"].to_numpy(), method=\"nearest\").to_numpy()\n",
    "    else:\n",
    "        ar = xr.DataArray(res[\"event_id\"].to_numpy(), dims=\"t\", coords=dict(t=res[\"t_shifted\"].to_numpy()))\n",
    "        target_ar = xr.DataArray(target_df[\"t_shifted\"].to_numpy(), dims=\"event_id\", coords=dict(event_id=target_df[\"event_id\"].to_numpy()))\n",
    "        match= ar.sel(t=target_ar, method=\"nearest\").to_dataframe(name=\"event_id\").reset_index(names=\"match\").drop(columns=\"t\")\n",
    "        res = pd.merge(res, match, how=\"left\", on=\"event_id\")\n",
    "    return res\n",
    "\n",
    "merge_data[\"match_df\"] = xr.apply_ufunc(match_event_df, merge_data[\"shifted_event_df\"], merge_data[\"shifted_event_df\"].isel(source=0), vectorize=True)\n",
    "merge_data[\"reconstructed_df\"] = xr.apply_ufunc(lambda dfs: pd.concat(dfs).sort_values(\"t_shifted\"), merge_data[\"match_df\"], input_core_dims=[[\"event\"]], vectorize=True)\n",
    "merge_data[\"order_changed\"] = xr.apply_ufunc(lambda init, rec: (init[\"event_id\"].to_numpy() != rec[\"event_id\"].to_numpy()).any(), merge_data[\"df\"], merge_data[\"reconstructed_df\"], vectorize=True)\n",
    "if merge_data[\"order_changed\"].any():\n",
    "    print(f'Possible problem for merge... order is not preserved')\n",
    "    display(merge_data[\"order_changed\"])\n",
    "merge_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_df = pd.concat([merge_data[\"reconstructed_df\"].isel(source=i).item().assign(source=merge_data[\"source\"].isel(source=i).item()) for i in range(merge_data.sizes[\"source\"])]).sort_values(\"t_shifted\")\n",
    "debug_df[\"event_id\"] = debug_df[\"event_id\"].where(debug_df[\"source\"] == merge_data[\"source\"].isel(source=0).item())\n",
    "print(debug_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events = merge_data[\"reconstructed_df\"].isel(source=0).item().assign(source=merge_data[\"source\"].isel(source=0).item()).drop(columns=[\"match\", \"t_start\"])\n",
    "for i in range(1, merge_data.sizes[\"source\"]):\n",
    "    d = merge_data[\"reconstructed_df\"].isel(source=i).item()\n",
    "    s = merge_data[\"source\"].isel(source=i).item()\n",
    "    d_matched = d.loc[~pd.isna(d[\"match\"])]\n",
    "    all_events=pd.merge(all_events, d_matched[[\"match\", \"metadata\", \"t_shifted\"]].rename(columns=dict(metadata=\"additional_metadata\", match=\"event_id\", t_shifted=\"other_t\")).assign(other_src=s), how=\"left\", on=\"event_id\")\n",
    "    try:\n",
    "        all_events[\"metadata\"] = all_events.apply(lambda row: \n",
    "                                          dict_merge(\n",
    "                                              row[\"metadata\"], \n",
    "                                              row[\"additional_metadata\"] if not pd.isna(row[\"additional_metadata\"]) else {},\n",
    "                                              {s:dict(t=row[\"other_t\"])}\n",
    "                                         ), axis=1)\n",
    "    except:\n",
    "        print(all_events.to_string())\n",
    "        raise\n",
    "    all_events[\"source\"] = np.where(~pd.isna(all_events[\"other_src\"]), all_events[\"source\"] + \"+\" +all_events[\"other_src\"], all_events[\"source\"])\n",
    "    all_events = all_events.drop(columns=[\"additional_metadata\", \"other_t\", \"other_src\"])\n",
    "    d_unmatched = d.loc[pd.isna(d[\"match\"])].copy().assign(source=s).drop(columns=[\"event_id\", \"match\", \"t_start\"])\n",
    "    all_events = pd.concat([all_events, d_unmatched])\n",
    "all_events = all_events.sort_values(\"t_shifted\").drop(columns=[\"event_id\"]).reset_index(drop=True).rename(columns=dict(t_shifted=\"t\"))\n",
    "all_events[\"metadata\"] = all_events.pop(\"metadata\")\n",
    "print(all_events.to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to = merge_data[\"source\"].isel(source=0).item()\n",
    "for i in range(1, merge_data.sizes[\"source\"]):\n",
    "    s = merge_data[\"source\"].isel(source=i).item()\n",
    "    data = all_events.copy()\n",
    "    data[\"other_t\"] = data[\"metadata\"].apply(lambda d: d[s][\"t\"] if s in d and \"t\" in d[s] else np.nan)\n",
    "    data[\"diff\"] = np.log10(1/(data[\"other_t\"] - data[\"t\"]).abs())\n",
    "    data = data.loc[~pd.isna(data[\"other_t\"])]\n",
    "    sns.displot(data, x=\"diff\", hue=\"event_name\", bins=5, multiple=\"dodge\", stat=\"probability\", common_norm=False, shrink=.8)\n",
    "    plt.suptitle(f'log10(1/abs(t_{s} - t_{compare_to})) after alignment distribution for each event type')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "nsources = len(merge_data[\"source\"])\n",
    "for i, s in enumerate(merge_data[\"source\"]):\n",
    "    for j, ev in enumerate(merge_data[\"event\"]):\n",
    "        df= merge_data[\"shifted_event_df\"].sel(source=s, event=ev).item()\n",
    "        other_drawargs = dict(label=ev.item()) if i==0 else {}\n",
    "        plt.vlines(df[\"t_shifted\"], [(i)]*len(df.index), [(i+1-0.02)]*len(df.index), color= f\"C{j}\", **other_drawargs)\n",
    "plt.yticks([0.5+i for i in range(merge_data.sizes[\"source\"])], merge_data[\"source\"].to_numpy())\n",
    "plt.legend()\n",
    "plt.suptitle(\"Events after alignment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing to trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_evs = set(all_events[\"event_name\"].dropna().to_list())\n",
    "trial_type_cols = set.union(*[set(d[\"trial_type\"].keys()) for d in all_events[\"metadata\"].values if \"trial_type\" in d])\n",
    "def reduce_trial(grp: pd.DataFrame): \n",
    "    trial = grp.name\n",
    "    prev_trial_end = grp[\"t\"].min()\n",
    "    real_start = grp.loc[grp[\"event_name\"]==\"trial_start\", \"t\"].max()\n",
    "    first_start = grp.loc[grp[\"event_name\"]==\"trial_start\", \"t\"].min()\n",
    "    if pd.isna(real_start):\n",
    "        real_grp = grp\n",
    "    else:\n",
    "        real_grp = grp.loc[grp[\"t\"] >=real_start]\n",
    "    events = real_grp[[\"event_name\", \"t\"]].dropna(subset=\"event_name\").set_index(\"event_name\")[\"t\"]\n",
    "    event_dict = {k:events[k] if k in events else pd.NA for k in all_evs}\n",
    "    if events.index.duplicated().any():\n",
    "        raise Exception(\"Duplicated events\")\n",
    "    \n",
    "    metadata = dict_merge(*real_grp[\"metadata\"].to_list(), incompatible=\"remove\")\n",
    "    trial_type = {k:metadata[\"trial_type\"][k] if \"trial_type\"  in metadata and k in metadata[\"trial_type\"] else pd.NA for k in trial_type_cols }\n",
    "    res = {\n",
    "           \"events\": pd.DataFrame([event_dict | dict(first_start=first_start, prev_trial_end = prev_trial_end)]),\n",
    "           \"trial_type\": pd.DataFrame([trial_type]),\n",
    "    }\n",
    "    # print(res)\n",
    "    r = pd.concat(res, axis=1)\n",
    "    r[\"trial\"] = trial\n",
    "    # print(r)\n",
    "    # raise Exception(\"stop\")\n",
    "    return r\n",
    "trial_event_df = all_events.groupby((all_events[\"event_name\"]==\"trial_end\").cumsum()-1).apply(reduce_trial).reset_index(drop=True)\n",
    "# test = trial_event_df[\"events\"].stack().to_xarray()\n",
    "trial_event_df[(\"events\", \"trial_end\")] = trial_event_df.pop((\"events\", \"prev_trial_end\")).shift(-1)\n",
    "trial_event_df=trial_event_df.set_index(\"trial\")\n",
    "\n",
    "print(trial_event_df.to_string())\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_dataset = xr.merge([trial_event_df[\"events\"].rename_axis('event_name', axis=1).stack().rename(\"event_t\").to_xarray(), trial_event_df[\"trial_type\"].to_xarray()])\n",
    "trial_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
